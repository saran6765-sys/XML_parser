{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Notebook: Test Transformation Strategies Step by Step\n",
    "\n",
    "This notebook validates the transformation helpers and utilities in this application step by step.\n",
    "\n",
    "Covered:\n",
    "- Expressions (EXP), Filters (FIL), Joins (JNR)\n",
    "- Lookups (LKP), Routers (RTR), Aggregations (AGG)\n",
    "- Sequence generation (SEQ), Merge (UPD_STR) — demo/optional\n",
    "- Validation helpers, AST extraction/normalization, logic derivation, code validator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and Spark session (local)\n",
    "import json\n",
    "import sys\n",
    "\n",
    "spark = None\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"research-strategy-tests\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    print(\"Spark version:\", spark.version)\n",
    "except Exception as e:\n",
    "    print(\"pyspark not available — Spark tests will be skipped.\")\n",
    "    print(e)\n",
    "\n",
    "# Import the local transformation framework utilities\n",
    "import importlib\n",
    "tfw = None\n",
    "try:\n",
    "    tfw = importlib.import_module(\"transform_framework\")\n",
    "    print(\"Loaded transform_framework.py\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load transform_framework.py\", e)\n",
    "\n",
    "# Import AST/validator agents utilities\n",
    "agt = None\n",
    "try:\n",
    "    agt = importlib.import_module(\"infa_to_pyspark_agents\")\n",
    "    print(\"Loaded infa_to_pyspark_agents.py\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load infa_to_pyspark_agents.py\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample DataFrames\n",
    "We create small DataFrames to exercise each helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "if spark:\n",
    "    tfw.setup_spark(spark, shuffle_partitions=8)\n",
    "\n",
    "    src_schema = T.StructType([\n",
    "        T.StructField(\"emp_id\", T.IntegerType(), False),\n",
    "        T.StructField(\"emp_name\", T.StringType(), True),\n",
    "        T.StructField(\"dept_id\", T.IntegerType(), True),\n",
    "        T.StructField(\"gross_sal\", T.DoubleType(), True),\n",
    "        T.StructField(\"tax_rate\", T.DoubleType(), True),\n",
    "        T.StructField(\"active_flg\", T.StringType(), True),\n",
    "        T.StructField(\"hire_date\", T.StringType(), True),\n",
    "    ])\n",
    "    src_rows = [\n",
    "        (1, \"Alice \t\", 10, 100000.0, 30.0, \"Y\", \"2020-01-01\"),\n",
    "        (2, \"Bob\", 10, 80000.0, 25.0, \"N\", \"2021-06-15\"),\n",
    "        (3, \"Charlie\", 20, 120000.0, 28.0, \"Y\", \"2019-11-30\"),\n",
    "    ]\n",
    "    df_src = spark.createDataFrame(src_rows, src_schema)\n",
    "\n",
    "    lkp_schema = T.StructType([\n",
    "        T.StructField(\"dept_id\", T.IntegerType(), False),\n",
    "        T.StructField(\"dept_name\", T.StringType(), True),\n",
    "        T.StructField(\"min_emp_id\", T.IntegerType(), True),\n",
    "        T.StructField(\"max_emp_id\", T.IntegerType(), True),\n",
    "    ])\n",
    "    lkp_rows = [\n",
    "        (10, \"Engineering\", 1, 100),\n",
    "        (20, \"Finance\", 100, 200),\n",
    "    ]\n",
    "    df_lkp = spark.createDataFrame(lkp_rows, lkp_schema)\n",
    "    df_src.show()\n",
    "    df_lkp.show()\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP: apply_expression\n",
    "- Trim names, compute TAX_AMT and NET_SAL, add LOAD_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    exprs = [\n",
    "        {\"name\": \"emp_name\", \"expr\": \"trim(emp_name)\"},\n",
    "        {\"name\": \"tax_amt\", \"expr\": \"gross_sal * tax_rate / 100\"},\n",
    "        {\"name\": \"net_sal\", \"expr\": \"gross_sal - (gross_sal * tax_rate / 100)\"},\n",
    "        {\"name\": \"load_ts\", \"expr\": \"current_timestamp()\"},\n",
    "    ]\n",
    "    df_exp = tfw.apply_expression(df_src, exprs)\n",
    "    df_exp.show()\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIL: apply_filter\n",
    "- Keep only active employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    df_fil = tfw.apply_filter(df_exp, \"active_flg = 'Y'\")\n",
    "    df_fil.show()\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JNR: apply_join\n",
    "- Join with department lookup by `dept_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    df_jnr = tfw.apply_join(df_fil, df_lkp.select(\"dept_id\", \"dept_name\"), on=[\"dept_id\"], how=\"left\")\n",
    "    df_jnr.show()\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LKP: apply_lookup_range\n",
    "- Range lookup by `emp_id` between `[min_emp_id, max_emp_id)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    df_lkp_range = tfw.apply_lookup_range(df_jnr, df_lkp, key=\"emp_id\", min_col=\"min_emp_id\", max_col=\"max_emp_id\", value_cols=[\"dept_name\"])\n",
    "    df_lkp_range.show()\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTR: apply_router\n",
    "- Split into salary buckets (LOW, MID, HIGH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    rules = [(\"LOW\", \"net_sal < 90000\"), (\"MID\", \"net_sal >= 90000 AND net_sal < 110000\"), (\"HIGH\", \"net_sal >= 110000\")]\n",
    "    routed = tfw.apply_router(df_lkp_range, rules)\n",
    "    {k: v.count() for k, v in routed.items()}\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGG: apply_aggregations\n",
    "- Total net salary by department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    df_agg = tfw.apply_aggregations(df_lkp_range, group_cols=[\"dept_id\"], agg_exprs={\"total_net\": \"sum(net_sal)\"})\n",
    "    df_agg.show()\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQ: add_sequence\n",
    "- Add a surrogate key by row_number() over all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    df_seq = tfw.add_sequence(df_lkp_range, col_name=\"emp_sk\", method=\"row_number\")\n",
    "    df_seq.select(\"emp_sk\", \"emp_id\", \"emp_name\").show()\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Helpers\n",
    "- Enforce schema, not-null, unique, surrogate key checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    expected = [(\"emp_id\", \"int\"), (\"emp_name\", \"string\"), (\"dept_id\", \"int\"), (\"gross_sal\", \"double\"), (\"tax_rate\", \"double\"), (\"active_flg\", \"string\"), (\"hire_date\", \"string\")]\n",
    "    df_enf = tfw.enforce_schema_df(df_src.select(\"emp_id\", \"emp_name\", \"dept_id\", \"gross_sal\", \"tax_rate\", \"active_flg\", \"hire_date\"), expected)\n",
    "    df_enf.printSchema()\n",
    "\n",
    "    issues = []\n",
    "    issues += tfw.validate_not_null_df(df_enf, [\"emp_id\"])\n",
    "    issues += tfw.validate_unique_df(df_enf, [\"emp_id\"])\n",
    "    issues += tfw.validate_surrogate_key_df(tfw.add_sequence(df_enf, \"emp_sk\"), \"emp_sk\")\n",
    "    print(\"Validation issues:\")\n",
    "    for i in issues: print(\"-\", i)\n",
    "    if not issues: print(\"No issues found.\")\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE Demo (optional)\n",
    "This requires Delta Lake support. On a vanilla local Spark without Delta, skip execution.\n",
    "In Databricks, set `target_table` to a managed Delta table name and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    try:\n",
    "        # Create small source df for merge and a target table if Delta is available\n",
    "        df_merge_src = df_seq.select(\"emp_id\", \"emp_name\").limit(2)\n",
    "        target_table = \"default.emp_merge_demo\"\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS default\")\n",
    "        # Try creating a Delta table (may fail if Delta not installed)\n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {target_table} (emp_id INT, emp_name STRING) USING delta\")\n",
    "        ok, msg = tfw.safe_delta_merge(\n",
    "            spark,\n",
    "            df_merge_src,\n",
    "            target_table=target_table,\n",
    "            on=\"t.emp_id = s.emp_id\",\n",
    "            update_set={\"emp_name\": \"s.emp_name\"},\n",
    "            insert_set={\"emp_id\": \"s.emp_id\", \"emp_name\": \"s.emp_name\"},\n",
    "            when_not_matched_insert=True,\n",
    "            overwrite_fallback=False,\n",
    "        )\n",
    "        print(\"MERGE result:\", ok, msg)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping MERGE demo — Delta likely unavailable.\")\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Skipping — Spark not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST Extraction and Normalization\n",
    "- Parse a tiny Informatica mapping XML and inspect the AST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_xml = \"\"\"<MAPPING NAME=\"m_demo\">\n",
    "  <SOURCE NAME=\"SRC_EMP\">\n",
    "    <SOURCEFIELD NAME=\"EMP_ID\" DATATYPE=\"integer\"/>\n",
    "  </SOURCE>\n",
    "  <TARGET NAME=\"TGT_EMP\">\n",
    "    <TARGETFIELD NAME=\"EMP_ID\" DATATYPE=\"integer\"/>\n",
    "  </TARGET>\n",
    "  <TRANSFORMATION NAME=\"FIL_ACTIVE\" TYPE=\"Filter\">\n",
    "    <TABLEATTRIBUTE NAME=\"Filter Condition\" VALUE=\"ACTIVE_FLG='Y'\"/>\n",
    "  </TRANSFORMATION>\n",
    "</MAPPING>\"\"\"\n",
    "\n",
    "if agt:\n",
    "    raw_ast = agt.extractor(sample_xml)\n",
    "    norm_ast = agt.normalizer(raw_ast)\n",
    "    print(\"AST:\")\n",
    "    print(raw_ast)\n",
    "    print(\n",
    "        \"\nNormalized AST:\\n\", norm_ast\n",
    "    )\n",
    "else:\n",
    "    print(\"Agents module not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Derivation and Code Validator\n",
    "- Derive logic from AST metadata and run the rule-based validator on a sample PySpark snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agt:\n",
    "    logic = agt.derive_logic(norm_ast)\n",
    "    print(\"Derived Logic:\")\n",
    "    print(logic)\n",
    "\n",
    "    sample_pyspark = \"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"A\")], ['emp_id','emp_name'])\n",
    "df.write.mode('overwrite').format('delta').saveAsTable('analytics.emp_payroll')\n",
    "\"\"\"\n",
    "    review = agt.validator(sample_pyspark, norm_ast, sql_code=None, intended_logic=\"keep active only\", extra_target_names=[\"emp_payroll\"])\n",
    "    print(\"Validator output:\")\n",
    "    print(review)\n",
    "else:\n",
    "    print(\"Agents module not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- For full MERGE and Delta operations, run this notebook on Databricks with Delta Lake.\n",
    "- The Streamlit app (`streamlit_app.py`) can generate a Databricks-style notebook that embeds these helpers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

